{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864637c0",
   "metadata": {},
   "source": [
    "\n",
    "# Apache Beam on Vertex AI Workbench — Clean Demo with `./artifacts`\n",
    "\n",
    "This notebook demonstrates the required Beam features on **Vertex AI Workbench** using the local **DirectRunner**, with **all inputs/outputs kept under a single `./artifacts` folder** for tidy execution and easy cleanup.\n",
    "\n",
    "**Features covered**\n",
    "- Pipeline I/O (`ReadFromText`, `WriteToText`)\n",
    "- `Map`, `Filter`\n",
    "- `ParDo` (custom `DoFn` with tagged side outputs)\n",
    "- `Partition`\n",
    "- Windowing (`FixedWindows`) + `CombinePerKey` and `WindowParam`\n",
    "- Composite transform (`PTransform`)\n",
    "\n",
    "**Run order**: top → bottom. Each section writes its results to `./artifacts`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca3289",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment setup & conventions\n",
    "\n",
    "- We install a compatible set known to work with Beam 2.69: `pyarrow==18.1.0`, `pandas<3`.\n",
    "- We **ignore Jupyter argv** so Beam doesn't warn about `-f kernel-....json`.\n",
    "- All file reads/writes use **`ARTIFACTS = Path.cwd() / \"artifacts\"`**.\n",
    "- Optional: install interactive extras if you want Beam's in-notebook visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca30ebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam: 2.69.0\n",
      "PyArrow: 18.1.0\n",
      "Python: 3.10.19 | Linux-5.10.0-36-cloud-amd64-x86_64-with-glibc2.31\n",
      "Artifacts directory: /home/jupyter/ApacheBeam/artifacts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Environment pins (Vertex-friendly) ---\n",
    "!pip install -q \"apache-beam[gcp,interactive]==2.69.0\" \"pandas<3\" \"pyarrow==18.1.0\"\n",
    "\n",
    "import logging, sys, platform\n",
    "import apache_beam as beam\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "\n",
    "# Make a clean artifacts dir\n",
    "ARTIFACTS = Path.cwd() / \"artifacts\"\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# Silence noisy arg parsing warnings from Jupyter:\n",
    "# Provide an empty argv list to PipelineOptions later.\n",
    "print(\"Beam:\", beam.__version__)\n",
    "print(\"PyArrow:\", pa.__version__)\n",
    "print(\"Python:\", sys.version.split()[0], \"|\", platform.platform())\n",
    "print(\"Artifacts directory:\", ARTIFACTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544c974",
   "metadata": {},
   "source": [
    "\n",
    "### Helper utilities\n",
    "\n",
    "Small helpers to list files and show the contents of single-shard outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01fa9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from glob import glob\n",
    "\n",
    "def list_artifacts(pattern=\"*\"):\n",
    "    base = str(ARTIFACTS / pattern)\n",
    "    files = sorted(glob(base))\n",
    "    print(f\"[artifacts/{pattern}] -> {len(files)} files\")\n",
    "    for f in files:\n",
    "        print(\" -\", Path(f).name)\n",
    "\n",
    "def first_match_text(prefix):\n",
    "    matches = sorted(glob(str(ARTIFACTS / f\"{prefix}-*-of-*.csv\"))) or               sorted(glob(str(ARTIFACTS / f\"{prefix}-*-of-*.txt\")))\n",
    "    if not matches:\n",
    "        print(f\"[no files for prefix={prefix}]\")\n",
    "        return\n",
    "    p = Path(matches[0])\n",
    "    print(f\"== {p.name} ==\")\n",
    "    print(p.read_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841852b6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Pipeline I/O + `Map` + `Filter` (clean CSV)\n",
    "\n",
    "We create a tiny input file in `./artifacts`, normalize rows with `Map`, drop blanks with `Filter`, and write a **single-shard** CSV to `./artifacts/cleaned-00000-of-00001.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c46808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifacts/cleaned*] -> 1 files\n",
      " - cleaned-00000-of-00001.csv\n",
      "== cleaned-00000-of-00001.csv ==\n",
      "Alpha,10\n",
      "Beta,7\n",
      "Gamma,13\n",
      "Delta,5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "# Create input file\n",
    "(ARTIFACTS / \"input.txt\").write_text(\"\\n\".join([\n",
    "    \"Alpha,10\",\n",
    "    \"Beta,7\",\n",
    "    \"\",\n",
    "    \"Gamma,  13  \",\n",
    "    \"Delta, 5\"\n",
    "]))\n",
    "\n",
    "def parse_row(line: str):\n",
    "    parts = [p.strip() for p in line.split(\",\")]\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    name, num = parts[0], parts[1]\n",
    "    try:\n",
    "        return name, int(num)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Tell Beam to ignore Jupyter argv\n",
    "pipeline_args = []\n",
    "opts = PipelineOptions(pipeline_args)\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    rows = (\n",
    "        p\n",
    "        | \"ReadFromText\" >> beam.io.ReadFromText(str(ARTIFACTS / \"input.txt\"))\n",
    "        | \"DropBlanks\" >> beam.Filter(lambda line: line.strip() != \"\")\n",
    "        | \"Parse\" >> beam.Map(parse_row)\n",
    "        | \"DropNone\" >> beam.Filter(lambda kv: kv is not None)\n",
    "    )\n",
    "    _ = (\n",
    "        rows\n",
    "        | \"ToCSV\" >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\")\n",
    "        | \"WriteCleaned\" >> beam.io.WriteToText(\n",
    "            str(ARTIFACTS / \"cleaned\"), file_name_suffix=\".csv\", num_shards=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "list_artifacts(\"cleaned*\")\n",
    "first_match_text(\"cleaned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2ee94",
   "metadata": {},
   "source": [
    "\n",
    "## 2) `ParDo` with tagged side outputs (big vs. small)\n",
    "\n",
    "We classify values ≥10 as **big** and others as **small** using a custom `DoFn` that emits **tagged outputs**. Results are written to `./artifacts/big-*.csv` and `./artifacts/small-*.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91791c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifacts/big*] -> 1 files\n",
      " - big-00000-of-00001.csv\n",
      "== big-00000-of-00001.csv ==\n",
      "('Alpha', 10)\n",
      "('Gamma', 13)\n",
      "\n",
      "[artifacts/small*] -> 1 files\n",
      " - small-00000-of-00001.csv\n",
      "== small-00000-of-00001.csv ==\n",
      "('Beta', 7)\n",
      "('Delta', 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apache_beam import DoFn, pvalue\n",
    "\n",
    "class SizeClassifier(DoFn):\n",
    "    def process(self, element):\n",
    "        name, val = element\n",
    "        if val >= 10:\n",
    "            yield pvalue.TaggedOutput(\"big\", (name, val))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput(\"small\", (name, val))\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    rows = (\n",
    "        p\n",
    "        | \"ReadCleaned\" >> beam.io.ReadFromText(str(ARTIFACTS / \"cleaned-*-of-*.csv\"))\n",
    "        | \"ParseCleaned\" >> beam.Map(lambda line: (line.split(\",\")[0], int(line.split(\",\")[1])))\n",
    "    )\n",
    "    outputs = rows | \"ClassifyBySize\" >> beam.ParDo(SizeClassifier()).with_outputs(\"big\", \"small\")\n",
    "    big, small = outputs.big, outputs.small\n",
    "\n",
    "    _ = big   | \"WriteBig\"   >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\") >> beam.io.WriteToText(str(ARTIFACTS / \"big\"),   file_name_suffix=\".csv\", num_shards=1)\n",
    "    _ = small | \"WriteSmall\" >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\") >> beam.io.WriteToText(str(ARTIFACTS / \"small\"), file_name_suffix=\".csv\", num_shards=1)\n",
    "\n",
    "list_artifacts(\"big*\")\n",
    "first_match_text(\"big\")\n",
    "list_artifacts(\"small*\")\n",
    "first_match_text(\"small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d8e37",
   "metadata": {},
   "source": [
    "\n",
    "## 3) `Partition` (split into 3 branches)\n",
    "\n",
    "We split the stream into **3 partitions** by `value % 3` and write outputs as CSV in `./artifacts/mod0|1|2-*.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaaac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifacts/mod0*] -> 1 files\n",
      " - mod0-00000-of-00001.csv\n",
      "== mod0-00000-of-00001.csv ==\n",
      "\n",
      "[artifacts/mod1*] -> 1 files\n",
      " - mod1-00000-of-00001.csv\n",
      "== mod1-00000-of-00001.csv ==\n",
      "('Alpha', 10)\n",
      "('Beta', 7)\n",
      "('Gamma', 13)\n",
      "\n",
      "[artifacts/mod2*] -> 1 files\n",
      " - mod2-00000-of-00001.csv\n",
      "== mod2-00000-of-00001.csv ==\n",
      "('Delta', 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def by_mod3(elem, n_partitions):\n",
    "    # elem: (name, value) -> route by value % 3\n",
    "    return elem[1] % 3\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    rows = (\n",
    "        p\n",
    "        | \"ReadCleanedCSV\" >> beam.io.ReadFromText(str(ARTIFACTS / \"cleaned-*-of-*.csv\"))\n",
    "        | \"Parse2\" >> beam.Map(lambda line: (line.split(\",\")[0], int(line.split(\",\")[1])))\n",
    "    )\n",
    "\n",
    "    p0, p1, p2 = rows | \"PartitionByMod3\" >> beam.Partition(by_mod3, 3)\n",
    "\n",
    "    _ = p0 | \"W0\" >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\") >> beam.io.WriteToText(str(ARTIFACTS / \"mod0\"), file_name_suffix=\".csv\", num_shards=1)\n",
    "    _ = p1 | \"W1\" >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\") >> beam.io.WriteToText(str(ARTIFACTS / \"mod1\"), file_name_suffix=\".csv\", num_shards=1)\n",
    "    _ = p2 | \"W2\" >> beam.Map(lambda kv: f\"{kv[0]},{kv[1]}\") >> beam.io.WriteToText(str(ARTIFACTS / \"mod2\"), file_name_suffix=\".csv\", num_shards=1)\n",
    "\n",
    "for prefix in (\"mod0\", \"mod1\", \"mod2\"):\n",
    "    list_artifacts(f\"{prefix}*\")\n",
    "    first_match_text(prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7c9ac",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Windowing: `FixedWindows(10s)` + `CombinePerKey`\n",
    "\n",
    "We simulate timestamped events, apply **10-second fixed windows**, sum values per user per window, and write results to `./artifacts/windowed-*.txt`. We also format the output to display window bounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f962dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifacts/windowed*] -> 1 files\n",
      " - windowed-00000-of-00001.txt\n",
      "== windowed-00000-of-00001.txt ==\n",
      "user=u1, total=7, window=[1761702950..1761702960)\n",
      "user=u1, total=7, window=[1761702960..1761702970)\n",
      "user=u2, total=6, window=[1761702960..1761702970)\n",
      "user=u2, total=1, window=[1761702970..1761702980)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from apache_beam.transforms.window import FixedWindows, TimestampedValue\n",
    "\n",
    "now = int(time.time())\n",
    "events = [\n",
    "    (\"u1\", 3, 0),\n",
    "    (\"u1\", 4, 5),\n",
    "    (\"u2\", 6, 12),\n",
    "    (\"u1\", 7, 13),\n",
    "    (\"u2\", 1, 19),\n",
    "]\n",
    "\n",
    "def to_timestamped(e):\n",
    "    user, val, offset = e\n",
    "    return TimestampedValue((user, val), now + offset)\n",
    "\n",
    "class FormatWithWindow(DoFn):\n",
    "    def process(self, element, window=DoFn.WindowParam):\n",
    "        user, total = element\n",
    "        yield f\"user={user}, total={total}, window=[{int(window.start)}..{int(window.end)})\"\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    formatted = (\n",
    "        p\n",
    "        | \"CreateEvents\" >> beam.Create(events)\n",
    "        | \"AttachTimestamps\" >> beam.Map(to_timestamped)\n",
    "        | \"WindowInto10s\" >> beam.WindowInto(FixedWindows(10))\n",
    "        | \"KeyByUser\" >> beam.Map(lambda uv: (uv[0], uv[1]))  # (user, value)\n",
    "        | \"SumPerUserPerWindow\" >> beam.CombinePerKey(sum)\n",
    "        | \"FormatRows\" >> beam.ParDo(FormatWithWindow())\n",
    "    )\n",
    "    _ = formatted | \"WriteWindowed\" >> beam.io.WriteToText(str(ARTIFACTS / \"windowed\"), file_name_suffix=\".txt\", num_shards=1)\n",
    "\n",
    "list_artifacts(\"windowed*\")\n",
    "first_match_text(\"windowed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6910a3d",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Composite transform (`PTransform`)\n",
    "\n",
    "We bundle cleaning, parsing, and tagging into a reusable `PTransform`, then write to `./artifacts/composite-*.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eae1a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[artifacts/composite*] -> 1 files\n",
      " - composite-00000-of-00001.txt\n",
      "== composite-00000-of-00001.txt ==\n",
      "{'name': 'Alpha', 'score': 10, 'is_big': True}\n",
      "{'name': 'Beta', 'score': 7, 'is_big': False}\n",
      "{'name': 'Gamma', 'score': 13, 'is_big': True}\n",
      "{'name': 'Delta', 'score': 5, 'is_big': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apache_beam import PTransform\n",
    "\n",
    "class CleanParseAndTag(PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | \"DropBlanks2\" >> beam.Filter(lambda line: line.strip() != \"\")\n",
    "            | \"Parse2\" >> beam.Map(lambda line: (line.split(\",\")[0].strip(), int(line.split(\",\")[1].strip())))\n",
    "            | \"TagScore\" >> beam.Map(lambda kv: {\"name\": kv[0], \"score\": kv[1], \"is_big\": kv[1] >= 10})\n",
    "        )\n",
    "\n",
    "with beam.Pipeline(options=opts) as p:\n",
    "    result = (\n",
    "        p\n",
    "        | \"ReadRawAgain\" >> beam.io.ReadFromText(str(ARTIFACTS / \"input.txt\"))\n",
    "        | \"CompositeTransform\" >> CleanParseAndTag()\n",
    "    )\n",
    "    _ = (\n",
    "        result\n",
    "        | \"ToStr\" >> beam.Map(lambda d: str(d))\n",
    "        | \"WriteCompositeOut\" >> beam.io.WriteToText(str(ARTIFACTS / \"composite\"), file_name_suffix=\".txt\", num_shards=1)\n",
    "    )\n",
    "\n",
    "list_artifacts(\"composite*\")\n",
    "first_match_text(\"composite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64703a98",
   "metadata": {},
   "source": [
    "\n",
    "## Cleanup\n",
    "\n",
    "Remove the entire `./artifacts` directory to reset the workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e19e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WARNING: this deletes all generated files for this demo.\n",
    "# Uncomment to clean up.\n",
    "import shutil\n",
    "shutil.rmtree(ARTIFACTS)\n",
    "print(\"Removed:\", ARTIFACTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c503ac",
   "metadata": {},
   "source": [
    "### ✅ Coverage Recap\n",
    "\n",
    "- **Pipeline I/O**: `ReadFromText`, `WriteToText` (Sections 1–5)  \n",
    "- **Map**: cleaning, parsing, formatting (all sections)  \n",
    "- **Filter**: removing blanks/invalid rows (Sections 1 & 5)  \n",
    "- **ParDo**: `SizeClassifier` (tagged outputs); `FormatWithWindow` using `WindowParam` (Sections 2 & 4)  \n",
    "- **Partition**: `beam.Partition` by modulo (Section 3)  \n",
    "- **Windowing**: `WindowInto(FixedWindows(10))` + `CombinePerKey` (Section 4)  \n",
    "- **Composite Transform**: `CleanParseAndTag` (`PTransform`) (Section 5)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "beam269",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python (Beam 2.69 / Arrow 18) (Local)",
   "language": "python",
   "name": "beam269"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
